{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from trainer import get_optimizer\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples, TTSDataset\n",
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
    "from TTS.tts.models.tacotron2 import Tacotron2\n",
    "from TTS.tts.models.glow_tts import GlowTTS\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "\n",
    "from cl_tts.benchmarks.dataset_formatters import vctk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ds_path = \"/raid/hhemati/Datasets/Speech/CL-TTS/VCTK/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:0.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:45\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    }
   ],
   "source": [
    "config = Tacotron2Config(\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"en-us\",\n",
    "    phoneme_cache_path=os.path.join(ds_path, \"phonemes\"),\n",
    "    use_d_vector_file=True,\n",
    "    d_vector_dim=256,\n",
    ")\n",
    "\n",
    "ap = AudioProcessor.init_from_config(config)\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "d_vectors_file_path = os.path.join(ds_path, \"speaker_embedding_means.json\")\n",
    "speaker_manager = SpeakerManager(d_vectors_file_path=d_vectors_file_path)\n",
    "\n",
    "model = Tacotron2(config, ap, tokenizer, speaker_manager=speaker_manager)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "Tacotron2Config(output_path='output', logger_uri=None, run_name='run', project_name=None, run_description='üê∏Coqui trainer run.', print_step=25, plot_step=100, model_param_stats=False, wandb_entity=None, dashboard_logger='tensorboard', log_model_step=None, save_step=10000, save_n_checkpoints=5, save_checkpoints=True, save_all_best=False, save_best_after=10000, target_loss=None, print_eval=False, test_delay_epochs=0, run_eval=True, distributed_backend='nccl', distributed_url='tcp://localhost:54321', mixed_precision=False, epochs=1000, batch_size=32, eval_batch_size=16, grad_clip=5.0, scheduler_after_epoch=True, lr=0.0001, optimizer='RAdam', optimizer_params={'betas': [0.9, 0.998], 'weight_decay': 1e-06}, lr_scheduler='NoamLR', lr_scheduler_params={'warmup_steps': 4000}, use_grad_scaler=False, cudnn_enable=True, cudnn_benchmark=True, torch_seed=54321, model='tacotron2', num_loader_workers=0, num_eval_loader_workers=0, use_noise_augment=False, use_language_weighted_sampler=False, audio=BaseAudioConfig(fft_size=1024, win_length=1024, hop_length=256, frame_shift_ms=None, frame_length_ms=None, stft_pad_mode='reflect', sample_rate=22050, resample=False, preemphasis=0.0, ref_level_db=20, do_sound_norm=False, log_func='np.log10', do_trim_silence=True, trim_db=45, do_rms_norm=False, db_level=None, power=1.5, griffin_lim_iters=60, num_mels=80, mel_fmin=0.0, mel_fmax=None, spec_gain=20, do_amp_to_db_linear=True, do_amp_to_db_mel=True, pitch_fmax=640.0, pitch_fmin=0.0, signal_norm=True, min_level_db=-100, symmetric_norm=True, max_norm=4.0, clip_norm=True, stats_path=None), use_phonemes=True, phonemizer=None, phoneme_language='en-us', compute_input_seq_cache=False, text_cleaner='phoneme_cleaners', enable_eos_bos_chars=False, test_sentences_file='', phoneme_cache_path='/raid/hhemati/Datasets/Speech/CL-TTS/VCTK/phonemes', characters=CharactersConfig(characters_class='TTS.tts.utils.text.characters.IPAPhonemes', vocab_dict=None, pad='<PAD>', eos='<EOS>', bos='<BOS>', blank='<BLNK>', characters='iy…® â…Øu…™ è äe√∏…ò…ô…µ…§o…õ≈ì…ú…û å…î√¶…êa…∂…ë…í·µª ò…ì«Ä…ó«É Ñ«Ç…†«Å õpbtd à…ñc…ük…°q…¢ î…¥≈ã…≤…≥n…±m ôr Ä‚±±…æ…Ω…∏Œ≤fvŒ∏√∞sz É í Ç ê√ß ùx…£œá Åƒß ïh…¶…¨…Æ ã…π…ªj…∞l…≠ é üÀàÀåÀêÀë çw…• ú ¢ °…ï ë…∫…ß ≤…öÀû…´', punctuations=\"!'(),-.:;? \", phonemes=None, is_unique=False, is_sorted=True), add_blank=False, batch_group_size=0, loss_masking=True, sort_by_audio_len=False, min_audio_len=1, max_audio_len=inf, min_text_len=1, max_text_len=inf, compute_f0=False, compute_linear_spec=False, precompute_num_workers=0, start_by_longest=False, datasets=[BaseDatasetConfig(name='', path='', meta_file_train='', ignored_speakers=None, language='', meta_file_val='', meta_file_attn_mask='')], test_sentences=[\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\", 'Be a voice, not an echo.', \"I'm sorry Dave. I'm afraid I can't do that.\", \"This cake is great. It's so delicious and moist.\", 'Prior to November 22, 1963.'], eval_split_max_size=None, eval_split_size=0.01, use_gst=False, gst=None, gst_style_input=None, num_speakers=1, num_chars=131, r=2, gradual_training=None, memory_size=-1, prenet_type='original', prenet_dropout=True, prenet_dropout_at_inference=False, stopnet=True, separate_stopnet=True, stopnet_pos_weight=10.0, max_decoder_steps=500, encoder_in_features=512, decoder_in_features=512, decoder_output_dim=80, out_channels=80, attention_type='original', attention_heads=None, attention_norm='sigmoid', attention_win=False, windowing=False, use_forward_attn=False, forward_attn_mask=False, transition_agent=False, location_attn=True, bidirectional_decoder=False, double_decoder_consistency=False, ddc_r=6, speakers_file=None, use_speaker_embedding=False, speaker_embedding_dim=512, use_d_vector_file=True, d_vector_file=False, d_vector_dim=256, seq_len_norm=False, decoder_loss_alpha=0.25, postnet_loss_alpha=0.25, postnet_diff_spec_alpha=0.25, decoder_diff_spec_alpha=0.25, decoder_ssim_alpha=0.25, postnet_ssim_alpha=0.25, ga_alpha=5.0)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizer and Criterion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(\n",
    "    optimizer_name=config.optimizer,\n",
    "    optimizer_params=config.optimizer_params,\n",
    "    lr=config.lr,\n",
    "    model=model,\n",
    ")\n",
    "criterion = model.get_criterion()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Found 44070 files in /raid/hhemati/Datasets/Speech/CL-TTS/VCTK\n",
      "43630\n"
     ]
    }
   ],
   "source": [
    "dataset_config = BaseDatasetConfig(\n",
    "    name=\"vctk\",  path=ds_path, meta_file_train=\"metadata.txt\"\n",
    ")\n",
    "train_samples, eval_samples = load_tts_samples(dataset_config, formatter=vctk)\n",
    "\n",
    "current_speakers = [\"vctk_p336\"]\n",
    "train_samples2 = [x for x in train_samples if x[\"speaker_name\"] in current_speakers]\n",
    "\n",
    "print(len(train_samples))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw_text': 'I want him to take on Gomez.', 'token_ids': array([  4,  64, 130,  25,  44,  16,  22, 130,  10,  64,  15, 130,  22,\n",
      "        49, 130,  22,   8,  64,  13, 130,  44,  16, 130,  56,  17,  90,\n",
      "        15,  51,  28, 126], dtype=int32), 'wav': array([-3.0517578e-05,  3.0517578e-05,  6.1035156e-05, ...,\n",
      "        4.5471191e-03,  4.5166016e-03,  4.3334961e-03], dtype=float32), 'pitch': None, 'attn': None, 'item_idx': '/raid/hhemati/Datasets/Speech/CL-TTS/VCTK/wavs/p323/p323_424.wav', 'speaker_name': 'vctk_p323', 'language_name': '', 'wav_file_name': 'p323_424.wav'}\n"
     ]
    }
   ],
   "source": [
    "samples = train_samples\n",
    "is_eval = False\n",
    "\n",
    "def get_dataset(samples, is_eval):\n",
    "    dataset = TTSDataset(\n",
    "        outputs_per_step=config.r if \"r\" in config else 1,\n",
    "        compute_linear_spec=config.model.lower() == \"tacotron\" or config.compute_linear_spec,\n",
    "        compute_f0=config.get(\"compute_f0\", False),\n",
    "        f0_cache_path=config.get(\"f0_cache_path\", None),\n",
    "        samples=samples,\n",
    "        ap=ap,\n",
    "        return_wav=config.return_wav if \"return_wav\" in config else False,\n",
    "        batch_group_size=0 if is_eval else config.batch_group_size * config.batch_size,\n",
    "        min_text_len=config.min_text_len,\n",
    "        max_text_len=config.max_text_len,\n",
    "        min_audio_len=config.min_audio_len,\n",
    "        max_audio_len=config.max_audio_len,\n",
    "        phoneme_cache_path=config.phoneme_cache_path,\n",
    "        precompute_num_workers=config.precompute_num_workers,\n",
    "        use_noise_augment=False if is_eval else config.use_noise_augment,\n",
    "        verbose=False,\n",
    "        # speaker_id_mapping=speaker_id_mapping,\n",
    "        # d_vector_mapping=d_vector_mapping if config.use_d_vector_file else None,\n",
    "        tokenizer=tokenizer,\n",
    "        start_by_longest=config.start_by_longest,\n",
    "        # language_id_mapping=language_id_mapping,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = get_dataset(train_samples, False)\n",
    "dataset.preprocess_samples()\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,  # shuffle is done in the dataset.\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    drop_last=False,  # setting this False might cause issues in AMP training.\n",
    "    sampler=None,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "print(dataset[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text_input'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [20]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(data_loader))\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext_input\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'text_input'"
     ]
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "print(batch[\"text_input\"].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Tacotron2(\n  (embedding): Embedding(131, 512, padding_idx=0)\n  (encoder): Encoder(\n    (convolutions): ModuleList(\n      (0): ConvBNBlock(\n        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout): Dropout(p=0.5, inplace=False)\n        (activation): ReLU()\n      )\n      (1): ConvBNBlock(\n        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout): Dropout(p=0.5, inplace=False)\n        (activation): ReLU()\n      )\n      (2): ConvBNBlock(\n        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout): Dropout(p=0.5, inplace=False)\n        (activation): ReLU()\n      )\n    )\n    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n  )\n  (decoder): Decoder(\n    (prenet): Prenet(\n      (linear_layers): ModuleList(\n        (0): Linear(\n          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n        )\n        (1): Linear(\n          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n        )\n      )\n    )\n    (attention_rnn): LSTMCell(1024, 1024)\n    (attention): OriginalAttention(\n      (query_layer): Linear(\n        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n      )\n      (inputs_layer): Linear(\n        (linear_layer): Linear(in_features=768, out_features=128, bias=False)\n      )\n      (v): Linear(\n        (linear_layer): Linear(in_features=128, out_features=1, bias=True)\n      )\n      (location_layer): LocationLayer(\n        (location_conv1d): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n        (location_dense): Linear(\n          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n        )\n      )\n    )\n    (decoder_rnn): LSTMCell(1792, 1024)\n    (linear_projection): Linear(\n      (linear_layer): Linear(in_features=1792, out_features=160, bias=True)\n    )\n    (stopnet): Sequential(\n      (0): Dropout(p=0.1, inplace=False)\n      (1): Linear(\n        (linear_layer): Linear(in_features=1184, out_features=1, bias=True)\n      )\n    )\n  )\n  (postnet): Postnet(\n    (convolutions): ModuleList(\n      (0): ConvBNBlock(\n        (convolution1d): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout): Dropout(p=0.5, inplace=False)\n        (activation): Tanh()\n      )\n      (1): ConvBNBlock(\n        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout): Dropout(p=0.5, inplace=False)\n        (activation): Tanh()\n      )\n      (2): ConvBNBlock(\n        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout): Dropout(p=0.5, inplace=False)\n        (activation): Tanh()\n      )\n      (3): ConvBNBlock(\n        (convolution1d): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n        (batch_normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout): Dropout(p=0.5, inplace=False)\n        (activation): Tanh()\n      )\n      (4): ConvBNBlock(\n        (convolution1d): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n        (batch_normalization): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout): Dropout(p=0.5, inplace=False)\n        (activation): Identity()\n      )\n    )\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = model.format_batch(batch)\n",
    "\n",
    "speaker_embeddings = [model.speaker_manager.get_d_vectors_by_speaker(spk) for spk in batch[\"speaker_names\"]]\n",
    "speaker_embeddings = torch.FloatTensor(speaker_embeddings).squeeze(1)\n",
    "\n",
    "batch[\"d_vectors\"] = speaker_embeddings.to(device)\n",
    "\n",
    "for k in batch.keys():\n",
    "    if isinstance(batch[k], torch.Tensor):\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhemati/anaconda3/envs/cl/lib/python3.8/site-packages/TTS/tts/models/tacotron2.py:276: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  alignment_lengths = mel_lengths // self.decoder.r\n",
      "/home/hhemati/anaconda3/envs/cl/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "outputs, loss_dict = model.train_step(batch, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "loss_dict[\"loss\"].backward()\n",
    "# if grad_clip > 0:\n",
    "#     grad_norm = torch.nn.utils.clip_grad_norm_(self.master_params(optimizer), grad_clip)\n",
    "optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.inference(batch[\"token_id\"][0].unsqueeze(0),\n",
    "#                 {\"d_vectors\": speaker_embeddings[0].unsqueeze(0)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}